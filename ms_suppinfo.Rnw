
<<preload, echo=FALSE>>=
library(knitr)
opts_chunk$set(echo=FALSE)
@


\noindent\textbf{Goal:} In our meta-analysis, reported studies differed in design as well as in vegetation type, biome or species studied. Such diversity is commonly referred to as methodological heterogeneity, and may or may not be responsible for observed discrepancies in the results of the studies. The extent of heterogeneity might influence the results of the meta-analysis, and thus induce some difficulty in drawing overall conclusions (See \citealt{Higgins:2002iq}). It is therefore important to be able to quantify the extent of heterogeneity among a collection of studies.

\noindent\textbf{Methods:} A common way of addressing the extent of heterogeneity is the statistic $I^{2}$ (\citealt{Santos:2012gt}, originally defined by \citealt{Higgins:2002iq}). $I^{2}$ estimates the consistency of the results obtained from published studies used in our meta-analysis; it describes the percentage of variability in point estimates that is due to heterogeneity rather than sampling error. It can be interpreted as a ratio (0\% $<$ $I^{2}$ $<$ 100\%) not affected by the number of studies or the metric of the effect size in the analysis.

An $I^{2}$ near zero indicates that almost all of the dispersion will be attributed to random error, and any attempt to explain the variance is an attempt to explain something that is (by definition) random. By contrast, as $I^{2}$ moves away from zero we know that some of the variance is real and can potentially be explained by subgroup analysis or meta-regression.
There is no clear rule to cover definition for ``low'', ``moderate'' or ``high'' heterogeneity. \citealt{Higgins:2003hz} suggested some benchmarks for $I^{2}$ based on the survey of meta-analyses of clinical trials. They suggested that values on the order of 25\%, 50\% and 75\% might be considered as ``low'', ``moderate'' and ``high'', respectively. These suggestions are tentative; the interpretation of heterogeneity in a systematic review will depend critically on the size and direction of treatment effects, as well as on considerations of methodological diversity in the studies (see \citealt{Borenstein:2009um}).
Here, we estimated $I^{2}$ as following equation from \citealt{Higgins:2002iq} using the entire dataset, then we calculated how part of the heterogeneity may be due to the influence of moderators (stage and publication year) and finally we asked if the residual heterogeneity was statistically significant. In R, we used the package metafor  \citep{Viechtbauer-2010}.


\noindent\textbf{Results:} We observed high levels of heterogeneity for all traits (SLA :$I^{2}$ \Sexpr{round(fun_Heterogeneity.H(GCi[["SLA"]], ~stageRGR)$Hlevel,0)}\% CI: \Sexpr{round(fun_Heterogeneity.CI(GCi[["SLA"]])$random[7], 0)}-\Sexpr{round(fun_Heterogeneity.CI(GCi[["SLA"]])$random[11],0)}\%, WD :$I^{2}$ \Sexpr{round(fun_Heterogeneity.H(GCi[["WD"]], ~stage)$Hlevel,0)}\% CI: \Sexpr{round(fun_Heterogeneity.CI(GCi[["WD"]])$random[7], 0)}-\Sexpr{round(fun_Heterogeneity.CI(GCi[["WD"]])$random[11],0)}, Hmax: $I^{2}$ \Sexpr{round(fun_Heterogeneity.H(GCi[["Hmax"]], ~stageRGR)$Hlevel,0)}\% CI: \Sexpr{round(fun_Heterogeneity.CI(GCi[["Hmax"]])$random[7], 0)}-\Sexpr{round(fun_Heterogeneity.CI(GCi[["Hmax"]])$random[11],0)}, seed mass: $I^{2}$ \Sexpr{round(fun_Heterogeneity.H(GCi[["Seedmass"]], ~stage)$Hlevel,0)}\%, CI \Sexpr{round(fun_Heterogeneity.CI(GCi[["Seedmass"]])$random[7], 1)}-\Sexpr{round(fun_Heterogeneity.CI(GCi[["Seedmass"]])$random[11],1)}, Aarea: $I^{2}$ \Sexpr{round(fun_Heterogeneity.H(GCi[["Aarea"]], ~stage)$Hlevel,1)}\% CI: \Sexpr{round(fun_Heterogeneity.CI(GCi[["Aarea"]])$random[7], 0)}-\Sexpr{round(fun_Heterogeneity.CI(GCi[["Aarea"]])$random[11],0)}) which suggests that the correlations coefficient r between growth and traits are not uniform across the range of studies investigated. This result suggested that a large part of the variance is not explained by sampling error.

For SLA, seed mass and Hmax, \Sexpr{round(fun_Heterogeneity.H(GCi[["SLA"]], ~stageRGR)$Hexplained,1)}\%, \Sexpr{round(fun_Heterogeneity.H(GCi[["Seedmass"]], ~stage)$Hexplained,1)}\% and \Sexpr{round(fun_Heterogeneity.H(GCi[["Hmax"]], ~stageRGR)$Hexplained, 0)}\% respectively of the total amount of heterogeneity were accounted for by including the plant stage as a moderator. For the other traits, the plant stage explained little or no part of the heterogeneity observed across studies ( WD: \Sexpr{round(fun_Heterogeneity.H(GCi[["WD"]], ~stage)$Hexplained, 0)}\% and Aarea: \Sexpr{round(fun_Heterogeneity.H(GCi[["Aarea"]], ~stage)$Hexplained, 1)}\% ). These results were consistent with our expectations about a shift in the relationship between growth and traits across plant size for SLA, seed mass and Hmax, and the absence of an effect for WD and Aarea (see Theoretical expectation section). 

The test for residual heterogeneity was significant for all traits, possibly indicating that other moderators not considered in the model were influencing the correlation between growth and traits. This is not surprising because although the trait and growth measurements are standardized \citep{Cornelissen:2003gw}, studies differed in their design as well as in vegetation type, biome, and species studied. Indeed, a goal in the trait literature is to establish general patterns of correlation between growth and traits and to validate them across a large variety of conditions. Here, we did not explore the influence of other moderators, since our goal was to test clear hypotheses about the influence of plant size/stage, not to establish the best predictor of the correlation between growth and traits. Finally, since between study variation is significant, as indicated by heterogeneity analyses, the use of the random effect model was appropriate in our meta-analysis.
\clearpage

\subsection{Testing for publication bias}
\noindent\textbf{Goal:} Studies with significant results are more likely to be published. As a consequence, the studies in a meta-analysis may overestimate the true effect size because they are based on a biased sample of the target population of studies. A publication bias occurs when the probability of publication depends on the statistical significance of the effect. Here we tested if there is evidence of any bias, and if the effect size observed for each trait is an artefact of this bias.

\noindent\textbf{Methods:} Initially, we checked the dataset for publication bias with a visual assessment of funnel plots. A publication bias toward significant results creates an asymmetric funnel with small studies with non-significant effects missing from the mouth of the funnel on the side opposite to the true effect. While funnel plot are recommended to aid interpretation, there is a high risk of failing to detect actual publication bias (see \citealt{Koricheva:2013tz}) by using an exclusively visual assessment. In addition, funnel plots are not effective when the number of studies is small ($<30$) such as for Aarea and seed mass.
Next, we used the ``trim and fill'' method to estimate the number of studies missing from our meta-analysis. This method adjusts for funnel-plot asymmetry \citep{Duval:2000dg}.
Finally, we estimated the number of studies needed to overturn a result, using Rosenberg's fail-safe number \citep{Rosenberg:2005hk}. The fail-safe N is not based on funnel plot asymmetry. Rather, the Rosenberg method calculates the number of studies on-average null results that would need to be added to the given set of observed outcomes to reduce the significance level (p-value) of the weighted average effect \citep{Rosenberg:2005hk}. A significant meta-analytic result is robust if the fail-safe N is greater than 5k+10, where k is the number of studies already in the meta-analysis \citep{Rosenthal:1979do}.
In R, we used the package ``metafor''  \citep{Viechtbauer-2010}.

\noindent\textbf{Results:} As expected the correlation coefficient varied less as sample size increased (funnel plots in Fig. \ref{fig:figA7}). Funnel plots exhibited a typical funnel shape for all five functional traits analysed, showing no evidence for publication bias (Fig. \ref{fig:figA7}).
The ``trim and fill'' method detected some missing values in the entire dataset (but no missing values in the "ideal" dataset). In the entire dataset, the estimated number of missing value ranges from 0 to \Sexpr{fun_trim.and.fill_number(GCi[["Seedmass"]])$N} depending on the trait considered (Fig. \ref{fig:figA7}). The overall estimates measured with the fill in data changed from $0.5 \pm 0.07$ to \Sexpr{round(fun_trim.and.fill_number(GCi[["SLA"]])$estimate, 2)} $\pm$ \Sexpr{round(fun_trim.and.fill_number(GCi[["SLA"]])$se, 2)} for SLA and from $-0.48 \pm 0.11$ to \Sexpr{round(fun_trim.and.fill_number(GCi[["Seedmass"]])$estimate, 2)}$\pm$ \Sexpr{round(fun_trim.and.fill_number(GCi[["Seedmass"]])$se, 2)} for the seed mass, suggesting a lower correlation coefficient for SLA, and a higher for seed mass. Although the correlations estimated with the missing studies filled in were closer to zero than without, the trait effect is still statistically significant (CIs not overlapping zero; SLA CI \Sexpr{round(fun_trim.and.fill_number(GCi[["SLA"]])$CImin, 2)} to \Sexpr{round(fun_trim.and.fill_number(GCi[["SLA"]])$CImax, 2)}, Seed mass CI \Sexpr{round(fun_trim.and.fill_number(GCi[["Seedmass"]])$CImin, 2)} to \Sexpr{round(fun_trim.and.fill_number(GCi[["Seedmass"]])$CImax, 2)}).

The Rosenberg's Fail-safe (N) were \Sexpr{fun_fsn(GCi[["SLA"]])[2]}, \Sexpr{fun_fsn(GCi[["WD"]])[2]}, \Sexpr{fun_fsn(GCi[["Hmax"]])[2]}, \Sexpr{fun_fsn(GCi[["Seedmass"]])[2]}, \Sexpr{fun_fsn(GCi[["Aarea"]])[2]} for SLA, WD, Hmax, Seedmass, and Aarea, suggesting that a large number of studies with a mean correlation coefficient of 0 need to be added to the analysis before the cumulative correlation would become non-significant. N was greater than 5k+10 for all traits (120 $< N <$ 455).
These results suggest that the impact of publication bias is probably trivial in our meta-analyses.

\clearpage
\subsection{Exploring temporal changes in effect size}
\noindent\textbf{Goal:} Our meta-analysis combines data from studies published between 1983 and 2014. It has been shown that the magnitude of the effect size may change over time due to a publication bias (i.e. a lag to publish negative results), a change in the methodology or biological changes in the magnitude of the effect (see \citealt{Koricheva:2013hy}). Therefore, detecting such temporal changes may be important to the interpretation of the results of a meta-analysis.

\noindent\textbf{Methods:} In the trait literature the year of publication may be confounded with a methodological change. Indeed, in the early 90's studies mainly focused on seedlings, whereas today a broader range of plant sizes/stages are studied (Fig. \ref{fig:figA9}). As a consequence, we used publication year as a moderator in our analyses \citep{Zvereva:2008jm} across but also within stages for all traits.

\noindent\textbf{Results:} There was a change in the effect size with publication year for SLA ($p<0.0001$, estimates: $-0.04 \pm 0.009$ ), seed mass ($p = 0.011$, estimates: $0.041 \pm 0.016$ ) and Aarea ($p = 0.0102$, estimates: $-0.0453 \pm 0.0176$ ). Fig. \ref{fig:figA9} shows that for these three traits the publication years may be confounded with plant stage; correlation coefficients have been reported since 1993, but studies started to report data on adult stage only ten years later (around 2000-2007).

For SLA and seed mass, a publication year effect was not observed within seedling and sapling stages, but was identified for the adult stage (SLA $p = 0.003$, seed mass $p = 0.06$). For both SLA and seed mass, the magnitude of the effect size decreased with publication year (Fig. \ref{fig:figA9} a and d), suggesting we overestimated the magnitude of the effect size for these two traits. In reality, the main effect size obtained would be more close to zero or more negative for SLA and more positive for seed mass. These results are consistent with our hypotheses; we expected a negative or non-significant effect of SLA on growth in adult stage, and a non-significant or positive effect of seed mass on plant growth (see theoretical expectation).

For Aarea, publication year explained a part of the variation across stage but not within stage, and the magnitude of the effect size decreased with publication year. This pattern is classic feature in meta-analyses: \citealt{Koricheva:2013hy} showed that a majority of studies reported a decrease rather than an increase in the magnitude of effect size with publication year. Yet, here this effect did not lead to a reduction in statistical significance of the main effect size or to a change of sign. While overestimated, the main effect size for the correlation between growth and Aarea stays positive (Fig. \ref{fig:figA9} e).

The temporal changes were confounded with a plant stage effect in our meta-analysis, but they did not jeopardize the stability of our conclusions. The conclusions of a meta-analysis conducted in different years should not differ from those reported here.


